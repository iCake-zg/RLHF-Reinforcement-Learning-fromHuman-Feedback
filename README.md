
# Reinforcement Learning from Human Feedback (RLHF)

æœ¬ä»“åº“å®ç°å¹¶è®°å½•äº† **äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰** çš„å®Œæ•´æµç¨‹ï¼Œæ¶µç›–ä¸‰å¤§æ ¸å¿ƒé˜¶æ®µï¼š

1. Supervised Fine-Tuning (SFT)
2. Reward Model Training (RM)
3. Policy Optimization (PPO)

![RLHFæµç¨‹å›¾](./assets/rlhf_diagram.png) <!-- è¯·ç¡®ä¿å›¾åƒè·¯å¾„æ­£ç¡® -->

## ğŸ é£ä¹¦ï¼š
https://jslv90rgc1.feishu.cn/wiki/N8pkwxPDfi431nk6jiwcAvHUnBb?from=from_copylink

---

## ğŸ“Œ ç›®å½•

- [Reinforcement Learning from Human Feedback (RLHF)](#reinforcement-learning-from-human-feedback-rlhf)
  - [ğŸ é£ä¹¦ï¼š](#é£ä¹¦)
  - [ğŸ“Œ ç›®å½•](#-ç›®å½•)
  - [1. Supervised Fine-Tuning (SFT)](#1-supervised-fine-tuning-sft)
    - [âœ… å†…å®¹ï¼š](#-å†…å®¹)
    - [ğŸ“Šæ•°æ®é›†ï¼štext](#æ•°æ®é›†text)
  - [2. Reward Model Training (RM)](#2-reward-model-training-rm)
    - [âœ… å†…å®¹ï¼š](#-å†…å®¹-1)
  - [](#)
  - [3. Policy Optimization (PPO)](#3-policy-optimization-ppo)
    - [âœ… å†…å®¹ï¼š](#-å†…å®¹-2)
  - [ğŸ“ é¡¹ç›®ç»“æ„è¯´æ˜ï¼ˆå»ºè®®ï¼‰](#-é¡¹ç›®ç»“æ„è¯´æ˜å»ºè®®)
  - [ğŸ™‹ Question](#-question)
    - [Q1:](#q1)
    - [Q2:](#q2)
  - [ğŸ–‡ï¸ KEYWORD](#ï¸-keyword)
    - [model.vocab\_size](#modelvocab_size)
    - [tokenizer.vocab\_size](#tokenizervocab_size)
    - [DataCollatorForLanguageModelingï¼ˆé¢„è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰](#datacollatorforlanguagemodelingé¢„è®­ç»ƒæ—¶ä½¿ç”¨)
  - [ğŸ“„ License](#-license)

---

## 1. Supervised Fine-Tuning (SFT)

> **ç›®æ ‡**ï¼šä½¿ç”¨äººç±»æ ‡æ³¨çš„é«˜è´¨é‡æ•°æ®ï¼Œè®©æ¨¡å‹å­¦ä¼šåŸºç¡€çš„â€œå¥½è¡Œä¸ºâ€ã€‚

### âœ… å†…å®¹ï¼š

- **æ•°æ®æ ¼å¼**ï¼š

```python
{
"prompt": "...",
"response": "..."
}
```

- **æŸå¤±å‡½æ•°**ï¼š
- Cross Entropy Lossï¼ˆäº¤å‰ç†µï¼‰

- **è®­ç»ƒæ–¹å¼**ï¼š
- é€šå¸¸åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œç»§ç»­è®­ç»ƒ

### ğŸ“Šæ•°æ®é›†ï¼š[text](configs/datasets/Belle_open_source_0.5M.json)
- æ•°æ®æ¨¡å¼ï¼š
æºè‡ª Stanford-Alpaca æœ€æ—©å¼€æºçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ç»“æ„ï¼Œåæ¥è¢« Belleã€Vicunaã€Open-Assistant ç­‰ä¸­æ–‡ç¤¾åŒºç›´æ¥æ²¿ç”¨

| é”®å            | ä½œç”¨              | åœºæ™¯ç¤ºä¾‹             |
| --------------- | --------------- | ---------------- |
| **instruction** | æè¿°â€œä»»åŠ¡ç±»å‹â€æˆ–â€œé«˜é˜¶æ„å›¾â€ | â€œå°†ä¸‹åˆ—è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡â€|
| **input**       | çœŸæ­£çš„â€œå¾…å¤„ç†å†…å®¹â€      | â€œI love apples.â€ |
| **output**      | æœŸæœ›ç­”æ¡ˆ            | â€œæˆ‘çˆ±è‹¹æœã€‚â€          |

- ä¸ºä»€ä¹ˆinputä¸ºç©ºï¼Ÿ
  - ç®€åŒ–æ ‡æ³¨ï¼šå¾ˆå¤šä»»åŠ¡ï¼ˆå¼€æ”¾å¼é—®ç­”ã€åˆ›æ„å†™ä½œã€å¸¸è¯†æ¨ç†ï¼‰æœ¬æ¥å°±åªæœ‰ä¸€å¥è¯æŒ‡ä»¤ï¼Œæ²¡å¿…è¦å†æ‹†å‡ºç¬¬äºŒä¸ªå­—æ®µã€‚
  - å…¼å®¹æ—§è„šæœ¬ï¼šæ—©æœŸå¼€æºä»“åº“ï¼ˆå¦‚ alpaca-loraã€Chinese-LLaMA-Alpacaï¼‰çš„ collator é»˜è®¤æŠŠ instruction å’Œ input æ‹¼æˆä¸€æ¡ prompt




---

## 2. Reward Model Training (RM)

> **ç›®æ ‡**ï¼šæ•™ä¼šæ¨¡å‹å­¦ä¼šåˆ¤æ–­å“ªä¸€ä¸ªå›ç­”æ›´å¥½ã€‚

### âœ… å†…å®¹ï¼š

- **æ•°æ®æ ¼å¼**ï¼š
```python
{
"prompt": "...",
"chosen": "...",
"rejected": "..."
}
```
- æ•°æ®é›†åç§°ï¼šHuggingFaceH4/ultrafeedback_binarized
- æ•°æ®é“¾æ¥ï¼šhttps://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized
- æ•°æ®é›†å®ä¾‹ï¼š
```python
{
  "prompt": "how can i develop a habit of drawing daily",
  "prompt_id": "086b3e24f29b8956a01059f79c56db35d118a06fb6b844b095737d042795cd43",
  "chosen": [
    {
      "content": "how can i develop a habit of drawing daily",
      "role": "user"
    },
    {
      "content": "Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\n\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\n3. Start small and simple: Don\\u2019t try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you\\u2019ve come.\n\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you\\u2019ll develop a habit of daily drawing that brings you joy and satisfaction.",
      "role": "assistant"
    }
  ],
  "rejected": [
    {
      "content": "how can i develop a habit of drawing daily",
      "role": "user"
    },
    {
      "content": "As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\n\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\n\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\n\n3. Make it fun: Don\\u2019t pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\n\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\n\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\n\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.",
      "role": "assistant"
    }
  ],
  "messages": [
    {
      "content": "how can i develop a habit of drawing daily",
      "role": "user"
    },
    {
      "content": "Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\n\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\n3. Start small and simple: Don\\u2019t try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you\\u2019ve come.\n\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you\\u2019ll develop a habit of daily drawing that brings you joy and satisfaction.",
      "role": "assistant"
    }
  ],
  "score_chosen": 8.5,
  "score_rejected": 8.5
}
```
- datasets[0]åœ°å€ï¼šRLHF-Reinforcement-Learning-fromHuman-Feedback/reward_model/data[0]view.json

- **ç»“æ„è®¾è®¡**ï¼š
- æ¨¡å‹æ”¹åŠ¨ï¼šåªæ”¹åŠ¨ transformer.h.30/31.c_atten/c_proj ,å†»ç»“å…¶ä»–å±‚
- æ•°æ®è®¾è®¡ï¼š
```python
# CHOSEN
<|im_start|>user
how can i develop a habit of drawing daily<|im_end|>
<|im_start|>assistant
Developing a daily habit of drawing can be challenging but with consistent practice ...<|im_end|>

#REJECTED
<|im_start|>user
how can i develop a habit of drawing daily<|im_end|>
<|im_start|>assistant
As an AI language model, I cannot personally develop habits for you. But ...<|im_end|>
```


- **æŸå¤±å‡½æ•°**ï¼š
- Pairwise Ranking Lossï¼ˆæˆå¯¹æ’åºæŸå¤±ï¼‰
```python
def compute_reward_loss(rewards):
    """
    è®¡ç®—reward modelçš„preference loss
    rewards: shape (batch_size*2, 1) æˆ– (batch_size*2,)
    å‰åŠéƒ¨åˆ†æ˜¯chosençš„rewardsï¼ŒååŠéƒ¨åˆ†æ˜¯rejectedçš„rewards
    """
    batch_size = rewards.shape[0] // 2
    chosen_rewards = rewards[:batch_size]      # å‰åŠéƒ¨åˆ†ï¼šchosen
    rejected_rewards = rewards[batch_size:]    # ååŠéƒ¨åˆ†ï¼šrejected
    
    # Preference loss: chosenåº”è¯¥æ¯”rejectedå¾—åˆ†æ›´é«˜
    loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()
    return loss, chosen_rewards, rejected_rewards
```
- **ä»£ç ç»“æ„**
<pre lang="nohighlight">
  <code>## ğŸ“ é¡¹ç›®ç»“æ„è¯´æ˜ï¼ˆå»ºè®®ï¼‰ 
```bash rlhf/reward_model
              â”œâ”€â”€ qwen-RW-finetuned/ # å¥–åŠ±æ¨¡å‹è°ƒæ•´åçš„ä¿å­˜è·¯å¾„
              â”œâ”€â”€ data_process.py / # å¯¹æºæ•°æ®è¿›è¡Œå¤„ç†
              â”œâ”€â”€ data[0]view_after_tokenizer.json / # tokenizerä¹‹åçš„ç¬¬ä¸€æ¡æºæ•°æ®
              â”œâ”€â”€ data[0]view.json / # ç¬¬ä¸€æ¡æºæ•°æ®
              â”œâ”€â”€ dowanload_datasets.py / # ä¸‹è½½æ•°æ®é›†
              â”œâ”€â”€ lora_set_train_parameters.py / # LoRAçš„å‚æ•°è®¾ç½®ï¼ˆå¦‚æœåŠ å…¥LoRAï¼‰
              â”œâ”€â”€ model_infor_check.py / # æ¨¡å‹åŸºç¡€ä¿¡æ¯æ£€æŸ¥
              â”œâ”€â”€ reward_data_collator_for_RW.py # é’ˆå¯¹å¥–åŠ±æ¨¡å‹çš„æ•°æ®æ”¶é›†å™¨ 
              â”œâ”€â”€ set_train_parameters.py # ä¸º Transformer.Trainer è®¾ç½®çš„è®­ç»ƒå‚æ•° 
              â””â”€â”€ train.py  # ä¸»è®­ç»ƒæµç¨‹
``` 
  </code>
</pre>
---

## 3. Policy Optimization (PPO)

> **ç›®æ ‡**ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å·²æœ‰æ¨¡å‹ï¼Œä½¿å…¶è¾“å‡ºæ›´ç¬¦åˆäººç±»åå¥½ã€‚

### âœ… å†…å®¹ï¼š

- **å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼š
- PPOï¼ˆProximal Policy Optimizationï¼‰

- **è®­ç»ƒè¾“å…¥**ï¼š
- æ¨¡å‹ç”Ÿæˆå¤šä¸ªå›ç­” â†’ å¥–åŠ±æ¨¡å‹æ‰“åˆ† â†’ æ ¹æ®å¥–åŠ±è¿›è¡Œç­–ç•¥æ›´æ–°

- **ä¼˜åŒ–éš¾ç‚¹**ï¼š
- KL çº¦æŸä¸å¤šæ­¥é‡‡æ ·
- æ˜¾å­˜ä¼˜åŒ–ï¼ˆå¦‚ Memory-efficient PPOï¼‰

---

## ğŸ“ é¡¹ç›®ç»“æ„è¯´æ˜ï¼ˆå»ºè®®ï¼‰

<pre lang="nohighlight">
  <code>## ğŸ“ é¡¹ç›®ç»“æ„è¯´æ˜ï¼ˆå»ºè®®ï¼‰ 
```bash rlhf/ 
              â”œâ”€â”€ sft/ # SFTè®­ç»ƒè„šæœ¬ä¸æ•°æ® 
              â”œâ”€â”€ reward_model/ # å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° 
              â”œâ”€â”€ ppo/ # RLHFä¸»æµç¨‹ï¼ˆPPOä¼˜åŒ–ï¼‰ 
              â”œâ”€â”€ configs/ # é…ç½®æ–‡ä»¶ï¼ˆè®­ç»ƒ/æ¨¡å‹/æ—¥å¿—ï¼‰ 
              â”œâ”€â”€ assets/ # å›¾ç‰‡/å¯è§†åŒ–å†…å®¹ï¼ˆå¦‚rlhf_diagram.pngï¼‰ 
              â””â”€â”€ README.md ``` 
  </code>
</pre>


---

## ğŸš§ TODO

- [ ] æ”¯æŒQLoRAè¿›è¡Œä½èµ„æºè®­ç»ƒ
- [ ] é›†æˆ`trl`åº“å¿«é€Ÿæ„å»ºè®­ç»ƒæµç¨‹
- [ ] å¢åŠ å®éªŒæ—¥å¿—ä¸TensorBoardæ”¯æŒ
- [ ] å¤šæ¨¡å‹æ”¯æŒï¼ˆå¦‚ Qwen / LLaMA / Baichuanï¼‰


å¯åŠ¨å‰è¯·å…ˆè®¾ç½®hfé•œåƒåœ°å€
```bash
Linuxï¼š export HF_ENDPOINT=https://hf-mirror.com
```
ç”Ÿæˆæ–‡å­—ï¼šåœ¨sftæ–‡ä»¶å¤¹ä¸‹
```bash
CUDA_VISIBLE_DEVICES=0 python generate.py
```

sftè®­ç»ƒ
```bash
CUDA_VISIBLE_DEVICES=0 python train.py icake-zg-train
```


å¯¹äºsftè®­ç»ƒä¿å­˜çš„æ¨¡å‹æ–‡ä»¶å¤¹ä¸­ï¼Œåœ¨æ¨¡å‹æ–‡ä»¶ä¸­å¯¼å…¥æºæ¨¡å‹æ–‡ä»¶çš„pythonæ–‡ä»¶
```bash
cp -f "$SRC/modeling_qwen.py" "$DST/"
cp -f "$SRC/configuration_qwen.py" "$DST/"
cp -f "$SRC/tokenization_qwen.py" "$DST/"
touch "$DST/__init__.py"
```

ä¿®æ”¹"config.json"ä¸­çš„æŒ‡å‘
```json
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel",
    "AutoTokenizer":"tokenization_qwen.QWenTokenizer"
  },

  "vocab_size": 151851
```

## ğŸ™‹ Question
### Q1:
Qwené»˜è®¤æ²¡æœ‰eos_tokenå’Œpad_token,æ‰€ä»¥éœ€è¦æ‰‹åŠ¨æ·»åŠ è¿™ä¸¤ä¸ªå€¼
```python
tokenizer.pad_token = tokenizer.eos_token = "<|endoftext|>"
model.config.pad_token_id = tokenizer.convert_tokens_to_ids("<|endoftext|>")
```

### Q2:



## ğŸ–‡ï¸ KEYWORD

### model.vocab_size
- æ¥æºï¼šæ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„vocab_sizeï¼ˆéœ€è¦åš64å¯¹é½ï¼‰
- ä½œç”¨ï¼šå†³å®šæ¨¡å‹çš„Embeddingå±‚ï¼Œä»¥åŠLM HEADçš„å¤§å°ï¼Œå³æ¨¡å‹èƒ½ç›´æ¥å¤„ç†çš„token ID èŒƒå›´
- æ³¨æ„ï¼šå¦‚æœç»™tokenizeræ·»åŠ äº†æ–°çš„tokenä½†æ˜¯æ²¡æœ‰è°ƒç”¨ model.resize_token_embeddings(len(tokenizer))ï¼Œé‚£ä¹ˆ model.vocab_size ä¸ä¼šå˜ã€‚

### tokenizer.vocab_size
- æ¥æºï¼šåˆ†è¯å™¨çš„è¯è¡¨å¤§å° tokenizer.get_vocab()
- ä½œç”¨ï¼šåæ˜ å½“å‰åˆ†è¯å™¨èƒ½è¯†åˆ«çš„tokenæ€»æ•°
- æ³¨æ„ï¼šå¸¸è§çš„é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ model.vocab_sizeå’Œ tokenizer ä¸€æ ·å¤§
- tokenizerä¸­padã€bosã€eosã€unk token ID æ˜¯é€šè¿‡ç‰¹æ®Šå€¼æ¥åŒºåˆ†çš„
  - bos_token="[BOS]" pad_token="[PAD]" eos_token="[EOS]" unk_token="[UNK]"


### DataCollatorForLanguageModelingï¼ˆé¢„è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰

```python
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)
```
- MLM(Masked Language Modeling) æ˜¯BERTä¸€ç±»çš„åŒå‘æ¨¡å‹è®­ç»ƒæ–¹å¼ï¼ŒéšæœºæŠŠè¾“å…¥çš„ä¸€éƒ¨åˆ†tokenæ¢æˆ[mask]æ ‡ç­¾ï¼Œç„¶åå†è®©æ¨¡å‹è¿˜åŸè¿™äº›è¢«maskæ‰çš„tokenã€‚
- CLM(Causal Language Modeling) æ˜¯GPTä¸€ç±»çš„å•å‘æ¨¡å‹è®­ç»ƒæ–¹å¼ï¼Œè®©æ¨¡å‹é¢„æµ‹ç¬¬tä¸ªtokenæ—¶åªèƒ½çœ‹åˆ°t-1ä¸ªä½ç½®çš„ä¿¡æ¯ï¼Œè¿™æ ·è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹åªå…³æ³¨å‰åçš„ç›¸å…³æ€§ï¼Œä¸å…³æ³¨å•è¯ä¸å•è¯ä¹‹é—´çš„ç»„åˆå…³ç³»ã€‚CLMåœ¨æ¨ç†æ—¶ï¼Œæ¯ä¸ªtokenéƒ½æ˜¯æ ¹æ®ä¸Šæ–‡ç”Ÿæˆã€‚ä½†æ˜¯BERTåœ¨æ¨ç†æ—¶éœ€è¦å®Œæ•´çš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œç„¶åæ‰èƒ½è®¡ç®—æ¯ä¸ªtokençš„mask logitsã€‚



---

## ğŸ“„ License

MIT License Â© 2025 [gezhou@usc.edu]
University of Southern California















